{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fetch company news from Finnhub, visit each article URL, extract the\n",
    "main text, and save everything to a CSV.\n",
    "\n",
    "ðŸ”‘  Prereqs\n",
    "    pip install requests beautifulsoup4 lxml pandas tqdm\n",
    "\n",
    "!!!  Some sites block scraping or require JS.  BeautifulSoup works\n",
    "     fine for most plainâ€‘HTML pages, but expect occasional failures.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, csv\n",
    "from datetime import date, timedelta, datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- USER SETTINGS -----------------------------------\n",
    "API_TOKEN  = \"d1nn689r01qovv8kac70d1nn689r01qovv8kac7g\"  # Finnhub key\n",
    "SYMBOL     = \"AAPL\"           # Stock ticker\n",
    "DAYS_BACK  = 50               # Lookâ€‘back window\n",
    "MAX_ART    = 100               # How many articles to pull (keep low to start)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "to_date   = date.today()\n",
    "from_date = to_date - timedelta(days=DAYS_BACK)\n",
    "\n",
    "# ------- 1) Get news metadata from Finnhub ------------------------\n",
    "url     = \"https://finnhub.io/api/v1/company-news\"\n",
    "params  = {\"symbol\": SYMBOL,\n",
    "           \"from\":   from_date.isoformat(),\n",
    "           \"to\":     to_date.isoformat(),\n",
    "           \"token\":  API_TOKEN}\n",
    "\n",
    "resp = requests.get(url, params=params, timeout=10)\n",
    "resp.raise_for_status()\n",
    "news_items = resp.json()[:MAX_ART]   # trim to desired count\n",
    "\n",
    "print(f\"Pulled {len(news_items)} news items for {SYMBOL}\")\n",
    "\n",
    "# ------- 2) Helper: extract readable text from an article ---------\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}   # mimic browser\n",
    "\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "        # crude heuristics: grab <article> if present, else large <p> block\n",
    "        art_tag = soup.find(\"article\")\n",
    "        if art_tag:\n",
    "            text = \" \".join(p.get_text(\" \", strip=True)\n",
    "                            for p in art_tag.find_all(\"p\"))\n",
    "        else:\n",
    "            # fallback: top 10 longest paragraphs\n",
    "            paragraphs = sorted((p.get_text(\" \", strip=True)\n",
    "                                 for p in soup.find_all(\"p\")),\n",
    "                                 key=len, reverse=True)[:10]\n",
    "            text = \" \".join(paragraphs)\n",
    "\n",
    "        return text[:4000]   # keep CSV manageable\n",
    "    except Exception as e:\n",
    "        return f\"[scrape error: {e}]\"\n",
    "\n",
    "# ------- 3) Loop through articles, scrape, collect rows ----------\n",
    "rows = []\n",
    "for art in tqdm(news_items, desc=\"Scraping\"):\n",
    "    art_time = datetime.fromtimestamp(art[\"datetime\"])\n",
    "    row = {\n",
    "        \"datetime\": art_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "        \"headline\": art[\"headline\"],\n",
    "        \"source\"  : art[\"source\"],\n",
    "        \"url\"     : art[\"url\"],\n",
    "        \"text\"    : scrape_article(art[\"url\"])\n",
    "    }\n",
    "    rows.append(row)\n",
    "    time.sleep(0.7)          # polite delay so we donâ€™t hammer sites\n",
    "\n",
    "# ------- 4) Write to CSV -----------------------------------------\n",
    "out_file = f\"{SYMBOL}_news_{from_date}_{to_date}.csv\"\n",
    "pd.DataFrame(rows).to_csv(out_file, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "print(f\"\\nSaved {len(rows)} articles â†’ {out_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
